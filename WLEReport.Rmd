---
title: "Practical Machine Learning Assignment"
author: "Benjamin Estrade"
date: "21 August 2018"
output: 
  html_document: 
    keep_md: yes
    theme: spacelab
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Executive Summary

The goal of this project was to accurately predict a lack of form in a number of ways while preforming an exercise. 

The best preforming model was a random forest which preformed with an accuracy of 99%.

10-fold cross validation was used to create the model. 

Correct predictions in final testing data was 20 out of 20, 100%.

Originally, a cobination of models was planned to be used but given the effectiveness of the single model this was not required. 

#Introduction

This project will be analyzing data from accelerometers on the waist, bicept, 
wrist and dumbell. 

This data was collected during a set of 10 Unilateral Dumbbell Biceps Curl by 6 
different participants.

The participants were asked to make different types of errors while completing 
the set which are reflected by the different classes. Each class is as follows:

* Class A - exactly according to the specification 

* Class B - throwing the elbows to the front 

* Class C - lifting the dumbbell only halfway 

* Class D - lowering the dumbbell only halfway 

* Class E - throwing the hips to the front 

All measurements y is up and down, x is right and left and z is forward and 
backward

#Aim of report

To try and build a predictive model based on this data that will accuractely 
predict the way in which the exercise is conducted. 


#Assumptions and Concerns

Assumptions:

* The date time data shouldn't be used in the prediction model as well as the 
variable x and num window

* The user name can be used as part of the prediction model

#Analysis

##Retriving and importing the data

```{r, warning = FALSE}
#recording time the data was downloaded for reproducablity
Sys.time()

training.url <- 
        "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

testing.url <- 
        "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

if(!dir.exists("datafiles")){
        dir.create("datafiles")
}

if(!dir.exists("datafiles/training.csv")){
        download.file(training.url, "datafiles/training.csv")
}

if(!dir.exists("datafiles/testing.csv")){
        download.file(testing.url, "datafiles/testing.csv")
}

training <- read.csv("datafiles/training.csv")
testing <- read.csv("datafiles/testing.csv")
```

##Exploritory Analysis

Looking at the data.

More information about the dataset can be found here:

http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har

```{r}
dim(training)
str(training)
```

The summary data seems to be available when new window = yes. Otherwise it is 
empty.

Looking at which fields of the testing data are all NA. If this is the case we 
can't use these fields for prediction and they will be removed.

```{r}
testing.not.empty <- apply(testing, 2, function(x) length(x) != sum(is.na(x)))
testing.not.empty[1:20]
```

Removing the empty fields and data which can't be used for predicitions from the 
data sets

```{r}
testing <- testing[,testing.not.empty]
training <- training[,testing.not.empty]
vars.not.for.prediction = c(1,3,4,5,6,7)
testing <- testing[,-vars.not.for.prediction]
training <- training[,-vars.not.for.prediction]
```

Splitting testing set into a testing and validation set

```{r, warning=FALSE, message=FALSE}
library(caret)

set.seed(2051)
inTrain <- createDataPartition(y=training$classe, p=0.7, list = FALSE)
training.nv <- training[inTrain,]
validation <- training[-inTrain,]
training <- training.nv
rm(training.nv)
```

Reviewing a variety of models using 10 fold cross validation. Only models with an accuracy above 70% will be used in the combined model. 

```{r, cache = TRUE, results = "hide"}
#setting the validation method I will be using during these trails
train.control <- trainControl(method = "cv", number = 10)
#setting the seed
set.seed(1564)
#lda model
lda.model <- train(classe ~ ., data = training, method = "lda", trControl = train.control)
#lda with pca preprocessing
pca.model <- train(classe ~ ., data = training, method = "lda", preProcess = "pca", trControl = train.control)
#qda model
qda.model <- train(classe ~ ., data = training, method = "qda", trControl = train.control)
#boosting using trees
gbm.model <- train(classe ~ ., data = training, method = "gbm", trControl = train.control)
#random forest model
rf.model <- train(classe ~ ., data = training, method = "rf", trControl = train.control)
```

Looking at the effectiveness of the models on the validation data set

```{r}
lda.pred <- predict(lda.model, validation)
pca.pred <- predict(pca.model, validation)
qda.pred <- predict(qda.model, validation)
gbm.pred <- predict(gbm.model, validation)
rf.pred <- predict(rf.model, validation)
lda.ac <- confusionMatrix(validation$classe, lda.pred)$overall[1]
pca.ac <- confusionMatrix(validation$classe, pca.pred)$overall[1]
qda.ac <- confusionMatrix(validation$classe, qda.pred)$overall[1]
gbm.ac <- confusionMatrix(validation$classe, gbm.pred)$overall[1]
rf.ac <- confusionMatrix(validation$classe, rf.pred)$overall[1]
data.frame(lda = lda.ac, pca = pca.ac, qda = qda.ac, gbm = gbm.ac, rf = rf.ac)
```

#Conclusion

The random forest model is the best preforming on real data and at over 99% no combinition is required. Accurcacy of model on the validation set is as follows.

```{r}
confusionMatrix(validation$classe, rf.pred)
```

The predictions for the 20 testing points are as follows:

```{r}
predict(rf.model, testing)
```